import torch
import numpy as np
import torch.nn as nn
import sys
import argparse
import os
import time

# 确保你有正确的路径来导入你的自定义模块
sys.path.append('../')
import quantization as q
from fracbnn_cifar10 import resnet20


class LayerParam:
    def __init__(self):
        pass


def calculate_scaled_params(gamma, beta, mean, var):
    scaled_weight = gamma / torch.sqrt(var)
    scaled_bias = beta - (gamma * mean / torch.sqrt(var))
    return scaled_weight, scaled_bias


def extract_layer_params(model):
    layer_params = []

    for name, sub_module in model.named_modules():
        if isinstance(sub_module, (q.BinaryConv2d, q.PGBinaryConv2d, nn.Conv2d)):
            for param_name, param_tensor in sub_module.named_parameters(recurse=False):
                if param_name == "threshold":
                    param = LayerParam()
                    param.name = f"{name}.{param_name}".replace('.', '_')
                    param.type = 'threshold'
                    param.weight = param_tensor.detach().numpy()
                    layer_params.append(param)
            continue  # Skip convolutional layers
        elif isinstance(sub_module, nn.BatchNorm2d):
            param = LayerParam()
            param.name = name.replace('.', '_')
            param.type = 'bn'
            param.weight, param.bias = calculate_scaled_params(
                sub_module.weight.detach(),
                sub_module.bias.detach(),
                sub_module.running_mean.detach(),
                sub_module.running_var.detach()
            )
            layer_params.append(param)
        elif isinstance(sub_module, nn.Linear):
            param = LayerParam()
            param.name = name.replace('.', '_')
            param.type = 'linear'
            param.weight = sub_module.weight.detach().numpy()
            param.bias = sub_module.bias.detach().numpy()
            layer_params.append(param)
        else:
            for param_name, param_tensor in sub_module.named_parameters(recurse=False):
                param = LayerParam()
                param.name = f"{name}.{param_name}".replace('.', '_')
                param.type = 'other'
                param.weight = param_tensor.detach().numpy()
                layer_params.append(param)

    return layer_params


def pad_to_size(arr, target_shape):
    padded_arr = np.zeros(target_shape, dtype=np.float32)
    original_shape = arr.shape

    if len(original_shape) == 1:
        flat_arr = arr.flatten()
        rows = flat_arr.size // target_shape[1]
        cols = flat_arr.size % target_shape[1]
        padded_arr[:rows, :target_shape[1]] = flat_arr[:rows * target_shape[1]].reshape(rows, target_shape[1])
        if rows < target_shape[0] and cols > 0:
            padded_arr[rows, :cols] = flat_arr[rows * target_shape[1]:]
    elif len(original_shape) == 2:
        slices = tuple(slice(0, min(dim, target_dim)) for dim, target_dim in zip(original_shape, target_shape))
        padded_arr[slices] = arr
    else:
        flat_arr = arr.flatten()
        if flat_arr.size > target_shape[0] * target_shape[1]:
            raise ValueError(f"Array too large to pad to target shape: {original_shape}")
        rows = flat_arr.size // target_shape[1]
        cols = flat_arr.size % target_shape[1]
        padded_arr[:rows, :target_shape[1]] = flat_arr[:rows * target_shape[1]].reshape(rows, target_shape[1])
        if rows < target_shape[0] and cols > 0:
            padded_arr[rows, :cols] = flat_arr[rows * target_shape[1]:]

    return padded_arr


def write_layer_params(layer_params, output_path):
    if not os.path.exists(output_path):
        os.makedirs(output_path)

    with open(os.path.join(output_path, 'weights_fracnet_64.h'), 'w') as f:
        f.write(f'''/********************************************************************************
* Filename: layer_weights.h
* Date: {time.ctime()}
* Description: This file is generated by the script.
********************************************************************************/

#include "typedefs.h"

''')

        for param in layer_params:
            if param.type == 'bn':
                weight_padded = pad_to_size(param.weight.numpy(), (4, 16))
                bias_padded = pad_to_size(param.bias.numpy(), (4, 16))
                f.write(f"const FIX_WT {param.name}_weight_fix[4][16] = ")
                print_ndarray_recursion(weight_padded, lambda x: f"{x:.7f}", f)
                f.write(';\n')
                f.write(f"const FIX_WT {param.name}_bias_fix[4][16] = ")
                print_ndarray_recursion(bias_padded, lambda x: f"{x:.7f}", f)
                f.write(';\n')
            elif param.type == 'linear':
                f.write(f"const FIX_WT {param.name}_weight_fix[{param.weight.shape[0]}][{param.weight.shape[1]}] = ")
                print_ndarray_recursion(param.weight, lambda x: f"{x:.7f}", f)
                f.write(';\n')
                f.write(f"const FIX_WT {param.name}_bias_fix[{param.bias.shape[0]}] = ")
                print_ndarray_recursion(param.bias, lambda x: f"{x:.7f}", f)
                f.write(';\n')
            elif param.type == 'threshold':
                weight_padded = pad_to_size(param.weight, (4, 16))
                f.write(f"const FIX_WT {param.name}_fix[4][16] = ")
                print_ndarray_recursion(weight_padded, lambda x: f"{x:.7f}", f)
                f.write(';\n')
            else:
                weight_padded = pad_to_size(param.weight, (4, 16))
                f.write(f"const FIX_WT {param.name}_fix[4][16] = ")
                print_ndarray_recursion(weight_padded, lambda x: f"{x:.7f}", f)
                f.write(';\n')



def print_ndarray_recursion(arr, str_func=str, file=sys.stdout, stop=0):
    if not hasattr(arr, '__iter__') or len(arr.shape) == stop:
        print(str_func(arr), file=file, end='')
        return
    ends = '' if (len(arr.shape) == stop + 1) else '\n'
    print('{', file=file, end='')
    for i, item in enumerate(arr):
        print_ndarray_recursion(item, str_func, file, stop)
        if i != len(arr) - 1: print(',', file=file, end=ends)
    print(ends + '}', file=file, end='')


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-w', '--weight', required=True, help='.pt file name to load weights from')
    parser.add_argument('-o', '--output', default='output', help='Directory to save extracted weights')
    args = parser.parse_args()

    # Load the model and the state dict
    ptfile = torch.load(args.weight, map_location='cpu')

    print("Keys in the .pt file:", ptfile.keys())

    model = resnet20()
    state_dict = model.state_dict()

    for key in state_dict.keys():
        if key in ptfile:
            state_dict[key] = ptfile[key]
        else:
            print(f"Key {key} not found in the .pt file")

    model.load_state_dict(state_dict, strict=False)
    model.eval()

    layer_params = extract_layer_params(model)
    write_layer_params(layer_params, args.output)
